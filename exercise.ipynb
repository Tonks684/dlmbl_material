{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28350ad0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# A Generative Modelling Approach to Image translation\n",
    "Written by Samuel Tonks, Krull Lab, University of Birmingham, UK.<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Generative Modelling\n",
    "In this part of the exercise, we will tackle the same supervised image-to-image translation task but use an alternative approach. Here we will explore a generative modelling approach, specifically a conditional Generative Adversarial Network (cGAN). <br>\n",
    "\n",
    "The previous regression-based method learns a deterministic mapping from phase contrast to fluorescence. This results in a single virtual staining prediction to the image translation task which often leads to blurry results. Virtual staining is an ill-posed problem; given the phase contrast image, with inherent noise and lack of contrast between the background and the structure of interest, it can be very challenging to virtually stain from the phase contrast image alone. In fact, there is a distribution of possible virtual staining solutions that could come from the phase contrast.\n",
    "\n",
    "cGANs learn to map from the phase contrast domain to a distirbution of virtual staining solutions. This distribution can then be sampled from to produce virtual staining predictions that are no longer a compromise between possible solutions which can lead to improved sharpness and realism in the generated images. Despite these improvements, cGANs can be prown to 'hallucinations' in which the network instead of making a compromise when it does not know something (such as a fine-grain detail of the nuclei shape) it makes something up that looks very sharp and realistic. These hallucinations can appear very plausible, but in many cases to predict such details from the phase contrast is extremely challenging. This is why determining reliable evaluation criteria for the task at hand is very important when dealing with cGANs .<br>\n",
    "<br>\n",
    "<br>\n",
    "![Overview of cGAN](https://github.com/Tonks684/dlmbl_material/blob/main/imgs/GAN.jpg?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "At a high-level a cGAN has two networks; a generator and a discriminator. The generator is a fully convolutional network that takes the source image as input and outputs the target image. The discriminator is also a fully convolutional network that takes as input the source image concatentated with a real or fake image and outputs the probabilities of whether the real fluorescence image is real or whether the fake virtual stain image is fake as shown in the figure above.<br>\n",
    "\n",
    "The generator is trained to fool the discriminator into predicting a high probability that its generated outputs are real, and the discriminator is trained to distinguish between real and fake images. Both networks are trained using an adversarial loss in a min-max game, where the generator tries to minimize the probability of the discriminator correctly classifying its outputs as fake, and the discriminator tries to maximize this probability. It is typically trained until the discriminator can no longer determine whether or not the generated images are real or fake better than a random guess (p(0.5)).<br>\n",
    "\n",
    "We will be exploring [Pix2PixHD GAN](https://arxiv.org/abs/1711.11585) architecture, a high-resolution extension of a traditional cGAN adapted for our recent [virtual staining works](https://ieeexplore.ieee.org/abstract/document/10230501?casa_token=NEyrUDqvFfIAAAAA:tklGisf9BEKWVjoZ6pgryKvLbF6JyurOu5Jrgoia1QQLpAMdCSlP9gMa02f3w37PvVjdiWCvFhA). Pix2PixHD GAN improves upon the traditional cGAN by using a coarse-to-fine generator, a multi-scale discrimator and additional loss terms. The \"coarse-to-fine\" generator is composed of two sub-networks, both ResNet architectures that operate at different scales. As shown below the first sub-network (G1) generates a low-resolution image, which is then upsampled and concatenated with the source image to produce a higher resolution image. The multi-scale discriminator is composed of 3 networks that operate at different scales, each network is trained to distinguish between real and fake images at that scale using the same convolution kernel size. This leads to the convolution having a much wider field of view when the inputs are downsampled. The generator is trained to fool the discriminator at each scale. \n",
    "<br>\n",
    "<br>\n",
    "![Pix2PixGAN ](https://github.com/Tonks684/dlmbl_material/blob/main/imgs/Pix2pixHD_1.jpg?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "The additional loss terms include a feature matching loss (as shown below), which encourages the generator to produce images that are perceptually similar to the real images at each scale. As shown below for each of the 3 discriminators, the network takes seperaetly both phase concatenated with virtual stain and phase concatenated with fluorescence stain as input and as they pass through the network the feature maps obtained for each ith layer are extracted. We then minimize the loss which is the mean L1 distance between the feature maps obtained across each of the 3 discriminators and each ith layer. <br>\n",
    "![Feature Matching Loss Pix2PixHD GAN](https://github.com/Tonks684/dlmbl_material/blob/main/imgs/Pix2pixHD_2.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2d826",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Today, we will train a 2D image translation model using the Pix2PixHD GAN. We will use the same dataset of 301 fields of view (FOVs) of Human Embryonic Kidney (HEK) cells, each FOV has 3 channels (phase, membrane, and nuclei) as used in the previous section.This implementation is designed to model a single translation task at once. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577ad67",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "This part of the exercise is organized in 5 parts.<br>\n",
    "\n",
    "As you have already explored the data in the previous parts, we will focus on training and evaluating Pix2PixHD GAN. The parts are as follows:<br>\n",
    "\n",
    "* **Part 1** - Define dataloaders & walk through steps to train a Pix2PixHD GAN.<br>\n",
    "* **Part 2** - Load and assess a pre-trained Pix2PixGAN using tensorboard, discuss the different loss components and how new hyper-parameter configurations could impact performance.<br>\n",
    "* **Part 3** - Evaluate performance of pre-trained Pix2PixGAN using pixel-level and instance-level metrics.<br>\n",
    "* **Part 4** - Compare the performance of Viscy (regression-based) with Pix2PixHD GAN (generative modelling approach)<br>\n",
    "* **Part 5** - *BONUS*: Sample different virtual staining solutions from the GAN using [MC-Dropout](https://arxiv.org/abs/1506.02142) and explore the variability and subsequent uncertainty in the virtual stain predictions.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a5616",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Our guesstimate is that each of the parts will take ~1 hour. A reasonable Pix2PixHD GAN can be trained in ~3.5 hours on a typical AWS node, this notebook is designed to walk you through the training steps but load a pre-trained model and tensorboard session to ensure we can complete the exercise in the time allocated. During Part 2 or 3, you're free to train your own model using the steps we outline in part 1.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f28d0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <span style=\"color:black;\">04_image_translation_phd</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22268e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "title": "<a ></a>"
   },
   "source": [
    "# Part 1: Define dataloaders & walk through steps to train a Pix2PixHD GAN.\n",
    "---------\n",
    "The focus of this part of the exercise is on understanding a generative modelling approach to image translation, how to train and evaluate a cGAN, and explore some hyperparameters of the cGAN. \n",
    "\n",
    "Learning goals:\n",
    "\n",
    "- Load dataset and configure dataloader.\n",
    "- Configure Pix2PixHD GAN to train for translating from phase to nuclei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2207f4",
   "metadata": {
    "title": "Imports and paths"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage import metrics\n",
    "from tifffile import imread, imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import all the necessary hyperparameters and configurations for training.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.options.train_options import TrainOptions\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.options.test_options import TestOptions\n",
    "\n",
    "# Import Pytorch dataloader and transforms.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.data.data_loader_dlmbl import CreateDataLoader\n",
    "\n",
    "# Import the model architecture.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.models import create_model\n",
    "\n",
    "# Import helper functions for visualization and processing.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.util.visualizer import Visualizer\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.util import util\n",
    "\n",
    "# Import train script.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.train_dlmbl import train as train_model\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.test_dlmbl import inference as inference_model\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.test_dlmbl import sampling\n",
    "\n",
    "# Import the function to compute segmentation scores.\n",
    "from GAN_code.GANs_MI2I.segmentation_scores import gen_segmentation_scores\n",
    "# pytorch lightning wrapper for Tensorboard.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Initialize the default options and parse the arguments.\n",
    "opt = TrainOptions().parse()\n",
    "# Set the seed for reproducibility.\n",
    "util.set_seed(42)\n",
    "# Set the experiment folder name.\n",
    "translation_task = \"nuclei\"  # or \"cyto\" depending on your choice of target for virtual stain.\n",
    "opt.name = \"dlmbl_vsnuclei\"\n",
    "# Path to store all the logs.\n",
    "opt.checkpoints_dir = Path(f\"./training/\").expanduser()\n",
    "output_image_folder = Path(\"./data/04_image_translation/tiff_files/\").expanduser()\n",
    "# Initalize the tensorboard writer.\n",
    "writer = SummaryWriter(log_dir=opt.checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65a98d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 1.1 Load Dataset & Configure Dataloaders.<br>\n",
    "Having already downloaded and split our training, validation and test sets we now need to load the data into the model. We will use the Pytorch DataLoader class to load the data in batches. The DataLoader class is an iterator that provides a consistent way to load data in batches. We will also use the CreateDataLoader class to load the data in the correct format for the Pix2PixHD GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a248c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Dataset and Dataloaders.\n",
    "\n",
    "## Define Dataset & Dataloader options.\n",
    "opt.dataroot = output_image_folder\n",
    "opt.data_type = 16  # Data type of the images.\n",
    "opt.loadSize = 512  # Size of the loaded phase image.\n",
    "opt.input_nc = 1  # Number of input channels.\n",
    "opt.output_nc = 1  # Number of output channels.\n",
    "opt.resize_or_crop = \"none\"  # Scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop|none].\n",
    "opt.target = \"nuclei\"  # or \"cyto\" depending on your choice of target for virtual stain.\n",
    "\n",
    "# Load Training Set for input into model\n",
    "train_dataloader = CreateDataLoader(opt)\n",
    "dataset_train = train_dataloader.load_data()\n",
    "print(f\"Total Training Images = {len(train_dataloader)}\")\n",
    "\n",
    "# Load Val Set\n",
    "opt.phase = \"val\"\n",
    "val_dataloader = CreateDataLoader(opt)\n",
    "dataset_val = val_dataloader.load_data()\n",
    "print(f\"Total Validation Images = {len(val_dataloader)}\")\n",
    "opt.phase= \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eb3d1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Configure Pix2PixHD GAN and train to predict nuclei from phase.\n",
    "Having loaded the data into the model we can now train the Pix2PixHD GAN to predict nuclei from phase. We will use the following hyperparameters to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameters for the Generator.\n",
    "opt.ngf = 64  # Number of filters in the generator.\n",
    "opt.n_downsample_global = 4  # Number of downsampling layers in the generator.\n",
    "opt.n_blocks_global = 9  # Number of residual blocks in the generator.\n",
    "opt.n_blocks_local = 3  # Number of residual blocks in the generator.\n",
    "opt.n_local_enhancers = 1  # Number of local enhancers in the generator.\n",
    "\n",
    "# Define the parameters for the Discriminators.\n",
    "opt.num_D = 3  # Number of discriminators.\n",
    "opt.n_layers_D = 3  # Number of layers in the discriminator.\n",
    "opt.ndf = 32  # Number of filters in the discriminator.\n",
    "\n",
    "# Define general training parameters.\n",
    "opt.gpu_ids = [0] # GPU ids to use.\n",
    "opt.norm = \"instance\"  # Normalization layer in the generator.\n",
    "opt.use_dropout = \"\"  # Use dropout in the generator (fixed at 0.2).\n",
    "opt.batchSize = 8  # Batch size.\n",
    "\n",
    "# Create a visualizer to perform image processing and visualization\n",
    "visualizer = Visualizer(opt)\n",
    "\n",
    "\n",
    "# Here will first start training a model from scrach however we can continue to train from a previously trained model by setting the following parameters.\n",
    "opt.continue_train = False\n",
    "if opt.continue_train:\n",
    "    iter_path = os.path.join(opt.checkpoints_dir, opt.name, \"iter.txt\")\n",
    "    try:\n",
    "        start_epoch, epoch_iter = np.loadtxt(iter_path, delimiter=\",\", dtype=int)\n",
    "    except:\n",
    "        start_epoch, epoch_iter = 1, 0\n",
    "    print(\"Resuming from epoch %d at iteration %d\" % (start_epoch, epoch_iter))\n",
    "else:\n",
    "    start_epoch, epoch_iter = 1, 0\n",
    "    \n",
    "print('------------ Options -------------')\n",
    "for k, v in sorted(vars(opt).items()):\n",
    "    print('%s: %s' % (str(k), str(v)))\n",
    "print('-------------- End ----------------')\n",
    "\n",
    "# Initialize the model\n",
    "phase2nuclei_model = create_model(opt)\n",
    "# Define Optimizers for G and D\n",
    "optimizer_G, optimizer_D = (\n",
    "    phase2nuclei_model.module.optimizer_G,\n",
    "    phase2nuclei_model.module.optimizer_D,\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    opt,\n",
    "    phase2nuclei_model,\n",
    "    visualizer,\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    optimizer_G,\n",
    "    optimizer_D,\n",
    "    start_epoch,\n",
    "    epoch_iter,\n",
    "    writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123e9aa",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## A heads up of what to expect from the training...\n",
    "<br>\n",
    "The train_model function has been designed so you can see the different Pix2PixHD GAN loss components discussed in the introductory section of the exercise as well as additional performance measurements.<br> \n",
    "As previously mentioned, Pix2PixHD GAN has two networks; a generator and a discriminator. The generator is trained to fool the discriminator into predicting a high probability that its generated outputs are real, and the discriminator is trained to distinguish between real and fake images. Both networks are trained using an adversarial loss in a min-max game, where the generator tries to minimize the probability of the discriminator correctly classifying its outputs as fake, and the discriminator tries to maximize this probability. It is typically trained until the discriminator can no longer determine whether or not the generated images are real or fake better than a random guess (p(0.5)). After a we have iterated through all the training data, we validate the performance of the network on the validation dataset. <br>\n",
    "\n",
    "In light of this, we plot the discriminator predicted probabilities of a real fluorescnece image being real and virtual image being a fake image, for the training and validation datasets.<br>\n",
    "\n",
    "Both networks are also trained using the generator feature matching loss which encourages the generator to produce images that contain similar statistics to the real images at each scale. We also plot the feature matching L1 loss for the training and validation sets together to observe the performance and how the model is fitting the data.<br>\n",
    "\n",
    "In our implementation, in addition to the Pix2PixHD GAN loss components already described we stabalize the GAN training by adding an additional least-square loss term. This term stabalizes the training of the GAN by penalizing the generator for producing images that the discriminator is very confident (high probability) are fake. This loss term is added to the generator loss and is used to train the generator to produce images that are similar to the real images.\n",
    "We plot the least-square loss (Generator_Loss_GAN) for the training and validation sets together to observe the performance and how the model is fitting the data.<br>\n",
    "\n",
    "This implementation allows for the turning on/off of the least-square loss term by setting the opt.no_lsgan flag to the model options. As well as the turning off of the feature matching loss term by setting the opt.no_ganFeat_loss flag to the model options. Something you might want to explore in the next section!<br><br>\n",
    "\n",
    "Finally, we also plot the Peak-Signal-to-Noise-Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) for the training and validation sets together to observe the performance and how the model is fitting the data.<br>\n",
    "\n",
    "[PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio), is a widely used metric to assess the quality of the generated image compared to the target image. Formally. it measures the ratio between the maximum possible power of a signal and the power of the corrupting noise that affects the fidelity of its representation. Essentially, PSNR provides a quantitative measurement of the quality of an image after compression or other processing such as image translation. Unlike the Pearson-Coeffecient, when measuring how much the pixel values of the virtual stain deviate from the target nuceli stain the score is sensitive to changes in brightness and contrast which is required for necessary for evaluating virtual staining. PSNR values range from 0dB to upper bounds that rarely exceed 60 dB. Extremely high PSNR values (above 50 dB) typically indicate almost negligible differences between the images.<br>\n",
    "\n",
    "\n",
    "[SSIM](https://en.wikipedia.org/wiki/Structural_similarity), is a perceptual metric used to measure the similarity between two images. Unlike PSNR, which focuses on pixel-wise differences, SSIM evaluates image quality based on perceived changes in structural information, luminance, and contrast. SSIM values range from -1 to 1, where 1 indicates perfect similarity between the images. SSIM is a more robust metric than PSNR, as it takes into account the human visual system\"s sensitivity to structural information and contrast. SSIM is particularly useful for evaluating the quality of image translation models, as it provides a more accurate measure of the perceptual similarity between the generated and target images.<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283010a8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## Checkpoint 1\n",
    "\n",
    "Congratulations! You should now have a better understanding of how a conditional generative model works!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfcb12",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0,
    "title": "<a ></a>"
   },
   "source": [
    "\n",
    "\n",
    "# Part 2: Load & Assess trained Pix2PixGAN using tensorboard, discuss performance of the model.\n",
    "--------------------------------------------------\n",
    "Learning goals:\n",
    "- Load a pre-trained Pix2PixHD GAN model for either phase to nuclei or phase to cyto (lets start with phase to nuclei: dlmbl_vsnuclei)\n",
    "- Discuss the loss components of Pix2PixHD GAN and how they are used to train the model.\n",
    "- Evaluate the fit of the model on the train and validation datasets.\n",
    "\n",
    "In this part, we will evaluate the performance of the pre-trained model. We will begin by looking qualitatively at the model predictions, then dive into the different loss curves, as well as the SSIM and PSNR scores achieves on the validation set. We will explore the implications of different hyper-parameter combinations for the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"./GAN_code/GANs_MI2I/pre_trained/{opt.name}/\"\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784f10e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Qualitative evaluation:\n",
    "<br>\n",
    "We have visualised the model output for an unseen phase contrast image and the target, nuclei stain.<br><br>\n",
    "\n",
    "Please note down your thoughts about the following questions...\n",
    "<br><br>\n",
    "\n",
    "1.**What do you notice about the virtual staining predictions? How do they appear compared to the regression-based approach? Can you spot any hallucinations?** \n",
    "<br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Quantitative evaluation:\n",
    "\n",
    "2.**What do you notice about the probabilities of the discriminators? How do the values compare during training compared to validation?\n",
    "<br>\n",
    "<br> \n",
    "3. What do you notice about the probabilities of the discriminators? How do the values compare during training compared to validation?<br><br>\n",
    "4. What do you notice about the feature matching L1 loss?<br><br>\n",
    "5. What do you notice about the least-square loss?<br><br>\n",
    "6. What do you notice about the PSNR and SSIM scores? Are we over or underfitting at all?**<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64547d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## Checkpoint 2\n",
    "\n",
    "Congratulations! You should now have a better understanding the different loss components of Pix2PixHD GAN and how they are used to train the model. You should also have a good understanding of the fit of the model during training on the training and validation datasets.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7d9ad",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "# Part 3: Evaluate performance of the virtual staining on unseen data.\n",
    "--------------------------------------------------\n",
    "## Evaluate the performance of the model.\n",
    "We now look at the same metrics of performance of the previous model. We typically evaluate the model performance on a held out test data. \n",
    "\n",
    "Steps:\n",
    "- Define our model parameters for the pre-trained model (these are the same parameters as shown in earlier cells but copied here for clarity).\n",
    "- Load the test data.\n",
    "\n",
    "We will first load the test data using the same format as the training and validation data. We will then use the model to predict the nuclei channel from the phase image. We will then evaluate the performance of the model using the following metrics:\n",
    "\n",
    "Pixel-level metrics:\n",
    "- [Peak-Signal-to-Noise-Ratio (PSNR)](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio).\n",
    "- [Structural Similarity Index Measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity).\n",
    "\n",
    "Instance-level metrics:\n",
    "- [F1 score](https://en.wikipedia.org/wiki/F1_score). via [Cellpose](https://cellpose.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = TestOptions().parse(save=False)\n",
    "\n",
    "# Define the parameters for the dataset.\n",
    "opt.dataroot = output_image_folder\n",
    "opt.data_type = 16  # Data type of the images.\n",
    "opt.loadSize = 512  # Size of the loaded phase image.\n",
    "opt.input_nc = 1  # Number of input channels.\n",
    "opt.output_nc = 1  # Number of output channels.\n",
    "opt.target = \"cyto\"  # \"nuclei\" or \"cyto\" depending on your choice of target for virtual stain.\n",
    "opt.resize_or_crop = \"none\"  # Scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop|none].\n",
    "opt.batchSize = 1 # Batch size for training\n",
    "\n",
    "# Define the model parameters for the pre-trained model.\n",
    "\n",
    "# Define the parameters for the Generator.\n",
    "opt.ngf = 64  # Number of filters in the generator.\n",
    "opt.n_downsample_global = 4  # Number of downsampling layers in the generator.\n",
    "opt.n_blocks_global = 9  # Number of residual blocks in the generator.\n",
    "opt.n_blocks_local = 3  # Number of residual blocks in the generator.\n",
    "opt.n_local_enhancers = 1  # Number of local enhancers in the generator.\n",
    "\n",
    "# Define the parameters for the Discriminators.\n",
    "opt.num_D = 3  # Number of discriminators.\n",
    "opt.n_layers_D = 3  # Number of layers in the discriminator.\n",
    "opt.ndf = 32  # Number of filters in the discriminator.\n",
    "\n",
    "# Define general training parameters.\n",
    "opt.gpu_ids= [0]  # GPU ids to use.\n",
    "opt.norm = \"instance\"  # Normalization layer in the generator.\n",
    "opt.use_dropout = \"\"  # Use dropout in the generator (fixed at 0.2).\n",
    "opt.batchSize = 8  # Batch size.\n",
    "\n",
    "# Define loss functions.\n",
    "opt.no_vgg_loss = \"\"  # Turn off VGG loss\n",
    "opt.no_ganFeat_loss = \"\"  # Turn off feature matching loss\n",
    "opt.no_lsgan = \"\"  # Turn off least square loss\n",
    "\n",
    "# Additional Inference parameters\n",
    "opt.name = f\"dlmbl_vsnuclei\"\n",
    "opt.how_many = 112  # Number of images to generate.\n",
    "opt.checkpoints_dir = f\"./GAN_code/GANs_MI2I/pre_trained/\"  # Path to the model checkpoints.\n",
    "opt.results_dir = f\"./GAN_code/GANs_MI2I/pre_trained/{opt.name}/inference_results/\"  # Path to store the results.\n",
    "opt.which_epoch = \"latest\"  # or specify the epoch number \"40\"\n",
    "opt.phase = \"test\"\n",
    "\n",
    "opt.nThreads = 1  # test code only supports nThreads = 1\n",
    "opt.batchSize = 1  # test code only supports batchSize = 1\n",
    "opt.serial_batches = True  # no shuffle\n",
    "opt.no_flip = True  # no flip\n",
    "Path(opt.results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the test data.\n",
    "test_data_loader = CreateDataLoader(opt)\n",
    "test_dataset = test_data_loader.load_data()\n",
    "visualizer = Visualizer(opt)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = create_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe652896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate & save predictions in the results directory.\n",
    "inference_model(test_dataset, opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5828bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results for evaluation\n",
    "virtual_stain_paths = sorted([i for i in Path(opt.results_dir).glob(\"**/*.tiff\")])\n",
    "target_stain_paths = sorted([i for i in Path(f\"{output_image_folder}/{translation_task}/test/\").glob(\"**/*.tiff\")])\n",
    "phase_paths = sorted([i for i in Path(f\"{output_image_folder}/input/test/\").glob(\"**/*.tiff\")])\n",
    "assert (len(virtual_stain_paths) == len(target_stain_paths) == len(phase_paths)\n",
    "), \"Number of images do not match.\"\n",
    "\n",
    "# Create arrays to store the images.\n",
    "virtual_stains = np.zeros((len(virtual_stain_paths), 512, 512))\n",
    "target_stains = virtual_stains.copy()\n",
    "phase_images = virtual_stains.copy()\n",
    "# Load the images and store them in the arrays.\n",
    "for index, (v_path, t_path, p_path) in tqdm(\n",
    "    enumerate(zip(virtual_stain_paths, target_stain_paths, phase_paths))\n",
    "):\n",
    "    virtual_stain = imread(v_path)\n",
    "    phase_image = imread(p_path)\n",
    "    target_stain = imread(t_path)\n",
    "    # Append the images to the arrays.\n",
    "    phase_images[index] = phase_image\n",
    "    target_stains[index] = target_stain\n",
    "    virtual_stains[index] = virtual_stain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf2908",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.1 Visualise the results of the model on the test set.\n",
    "\n",
    "Create a matplotlib plot that visalises random samples of the phase images, target stains, and virtual stains.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a69b6",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## TODO ########\n",
    "##########################\n",
    "\n",
    "def visualise_results():\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66ff3e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.2 Compute pixel-level metrics\n",
    "\n",
    "Compute the pixel-level metrics for the virtual stains and target stains. The metrics include Pearson correlation, SSIM, and PSNR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pd.DataFrame(columns=[\"pearson_nuc\", \"SSIM_nuc\", \"psnr_nuc\"])\n",
    "# Pixel-level metrics\n",
    "for i, (target_image, predicted_image) in enumerate(zip(target_stains, virtual_stains)):\n",
    "    # Compute SSIM and pearson correlation.\n",
    "    ssim_score = metrics.structural_similarity(\n",
    "        target_image, predicted_image, data_range=1\n",
    "    )\n",
    "    pearson_score = np.corrcoef(target_image.flatten(), predicted_image.flatten())[0, 1]\n",
    "    psnr_score = metrics.peak_signal_noise_ratio(\n",
    "        target_image, predicted_image, data_range=1\n",
    "    )\n",
    "    test_metrics.loc[i] = {\n",
    "        \"pearson_nuc\": pearson_score,\n",
    "        \"SSIM_nuc\": ssim_score,\n",
    "        \"psnr_nuc\": psnr_score,\n",
    "    }\n",
    "\n",
    "test_metrics.boxplot(\n",
    "    column=[\"pearson_nuc\", \"SSIM_nuc\", \"psnr_nuc\"],\n",
    "    rot=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce78644",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.3 Compute instance-level metrics\n",
    "\n",
    "- Use Cellpose to segment the nuclei or  membrane channels of the fluorescence and virtual staining images.\n",
    "- Compute the F1 score for the segmentation masks.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999b395",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Run cellpose to generate masks for the virtual stains\n",
    "path_to_virtual_stain = Path(opt.results_dir)\n",
    "path_to_targets = Path(f\"{output_image_folder}/test/\")\n",
    "cellpose_model = \"nuclei\"  # or \"cyto\" depending on your choice of target for virtual stain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73eedc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Run for virtual stain\n",
    "!python -m cellpose --dir $path_to_virtual_stain --pretrained_model $cellpose_model --chan 0 --save_tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = sorted([i for i in path_to_virtual_stain.glob(\"**/*_cp_masks.tif*\")])\n",
    "target_masks = sorted([ifor i in Path('./data/nuclei/masks/).glob(\"**/*.tiff\")])\n",
    "assert len(predicted_masks) == len(target_masks), \"Number of masks do not match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7047f",
   "metadata": {},
   "source": [
    "Use a predefined function to compute F1 score and its component parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ecb69",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Generate dataframe to store the outputs\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        'Model', 'Image', 'GT_Cell_Count','Threshold', 'F1', 'IoU',\n",
    "        'TP', 'FP', 'FN', 'Precision', 'Recall'\n",
    "    ],\n",
    ") \n",
    "# Create inputs to function\n",
    "image_sets = []\n",
    "for i in range(len(predicted_masks)):\n",
    "    name = str(predicted_masks[i]).split(\"/\")[-1] \n",
    "    virtual_stain_mask = imread(predicted_masks[i])\n",
    "    fluorescence_mask = imread(target_masks[i])  \n",
    "    image_sets.append(\n",
    "        {\n",
    "            \"Image\": name,\n",
    "            \"Model\": \"Pix2PixHD\",\n",
    "            \"Virtual_Stain_Mask\": virtal_stain_mask,\n",
    "            \"Fluorescence_Mask\": fluorescence_mask,\n",
    "        }\n",
    "    )\n",
    "# Compute the segmentation scores\n",
    "results, _, _ = \\\n",
    "    gen_segmentation_scores(\n",
    "        image_sets, results, final_score_output=f\"./GAN_code/GANs_MI2I/pre_trained/{opt.name}/inference_results/\")\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95775f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Mean F1 results\n",
    "mean_f1 = results[\"F1\"].mean()\n",
    "std_f1 = results[\"F1\"].std()\n",
    "print(f\"Mean F1 Score: {np.round(mean_f1,2)}\")\n",
    "\n",
    "plt.hist(results[\"F1\"], bins=10)\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"F1 Score: Mu {mean_f1}+-{std_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b5ed0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## Checkpoint 3\n",
    "\n",
    "Congratulations! You have generated predictions from a pre-trained model and evaluated the performance of the model on unseen data. You have computed pixel-level metrics and instance-level metrics to evaluate the performance of the model. You may have also began training your own Pix2PixHD GAN models with alternative hyperparameters.\n",
    "Please document hyperparameters, snapshots of predictions on validation set, and loss curves for your models and add the final perforance in [this google doc](ADD LINK TO SHARED DOC). We\"ll discuss our combined results as a group.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f5909",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Part 4. Visualise Regression vs Generative Modelling Approaches\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff17bc",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Viscy Virtual Stains\n",
    "viscy_results_path = \"/ADD/PATH/TO/RESULTS/HERE\"\n",
    "viscy_stain_paths = sorted([i for i in Path(viscy_results_path).glob(\"**/*.tiff\")])\n",
    "assert len(viscy_stain_paths) == len(virtual_stain_paths), \"Number of images do not match.\"\n",
    "visy_stains = np.zeros((len(viscy_stain_paths), 512, 512))\n",
    "for index, v_path in enumerate(viscy_stain_paths):\n",
    "    viscy_stain = imread(v_path)\n",
    "    visy_stains[index] = viscy_stain\n",
    "\n",
    "##########################\n",
    "######## TODO ########\n",
    "##########################\n",
    "\n",
    "\n",
    "def visualise_both_methods():\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f14227",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Part 5: BONUS: Sample different virtual staining solutions from the GAN using MC-Dropout and explore the uncertainty in the virtual stain predictions.\n",
    "--------------------------------------------------\n",
    "Steps:\n",
    "- Load the pre-trained model.\n",
    "- Generate multiple predictions for the same input image.\n",
    "- Compute the pixel-wise variance across the predictions.\n",
    "- Visualise the pixel-wise variance to explore the uncertainty in the virtual stain predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same model and dataloaders as before.\n",
    "# Load the test data.\n",
    "test_data_loader = CreateDataLoader(opt)\n",
    "test_dataset = test_data_loader.load_data()\n",
    "visualizer = Visualizer(opt)\n",
    "\n",
    "# Load pre-trained model\n",
    "opt.variational_inf_runs = 100 # Number of samples per phase input\n",
    "opt.variation_inf_path = f\"./GAN_code/GANs_MI2I/pre_trained/{opt.name}/samples/\"  # Path to store the samples.\n",
    "opt.dropout_variation_inf = True  # Use dropout during inference.\n",
    "model = create_model(opt)\n",
    "# Generate & save predictions in the variation_inf_path directory.\n",
    "sampling(test_dataset, opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204fd3d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualise Samples                                      \n",
    "samples = sorted([i for i in Path(f\"./GAN_code/GANs_MI2I/pre_trained/{opt.name}/samples\").glob(\"**/*mask*.tif*\")])\n",
    "# Create arrays to store the images.\n",
    "sample_images = np.zeros((len(samples),112, 512, 512)) # (samples, images, height, width)\n",
    "# Load the images and store them in the arrays.\n",
    "for index, sample_path in tqdm(enumerate(samples)):\n",
    "    sample_image = imread(sample_path)\n",
    "    # Append the images to the arrays.\n",
    "    sample_images[index] = sample_image\n",
    "# Plot the phase image, the target image, the variance of samples and 3 samples\n",
    "\n",
    "# Create a matplotlib plot with animation through images.\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def animate_images(images):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    im = ax.imshow(images[0], cmap='gray')\n",
    "\n",
    "    def update(i):\n",
    "        im.set_array(images[i])\n",
    "        return im,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(images), interval=200)\n",
    "    plt.show()\n",
    "\n",
    "animate_images(sample_images)\n",
    "\n",
    "# Visualise the results of the model on the test set.\n",
    "fig, axes = plt.subplots(3, 7, figsize=(20, 5))\n",
    "sample_indices = np.random.choice(sample_images.shape[1], 3)\n",
    "for row, indices in enumerate(sample_indices):\n",
    "    axes[row, 0].imshow(phase_images[indices], cmap=\"gray\")\n",
    "    axes[row, 0].set_title(\"Phase\")\n",
    "    axes[row,0].axis(\"off\")\n",
    "    axes[row, 1].imshow(target_stains[indices], cmap=\"gray\")\n",
    "    axes[row, 1].set_title(\"Target Fluorescence\")\n",
    "    axes[row,1].axis(\"off\")\n",
    "    variance = np.var(sample_images[:,indices], axis=0)\n",
    "    axes[row, 2].imshow(variance, cmap=\"inferno\")\n",
    "    axes[row, 2].set_title(\"Pixel-wise Sample Variance\")\n",
    "    axes[row, 2].axis(\"off\")\n",
    "    for col in range(3, 7):\n",
    "        axes[row, col].imshow(sample_images[col-3,indices], cmap=\"gray\")\n",
    "        axes[row, col].set_title(f\"Sample {col-3}\")\n",
    "        axes[row,col].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()                          \n",
    "    \n",
    "                                      "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
